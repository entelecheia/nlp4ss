
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.2 Overview of Generative LLMs &#8212; NLP for Social Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'session01/lecture2';</script>
    <script src="../_static/language_switcher.js?v=ff97be19"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.3 Ethical Considerations and Challenges in Using LLMs for Research" href="lecture3.html" />
    <link rel="prev" title="1.1 Fundamentals of NLP and its Evolution" href="lecture1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">NLP for Social Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Session 1 - Introduction to NLP for Social Science</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lecture1.html">1.1 Fundamentals of NLP and its Evolution</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.2 Overview of Generative LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture3.html">1.3 Ethical Considerations and Challenges in Using LLMs for Research</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session02/index.html">Session 2: Traditional NLP Techniques and Text Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture1.html">2.1 Text Cleaning, Normalization, and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture2.html">2.2 Basic NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture3.html">2.3 Topic Modeling and Latent Dirichlet Allocation (LDA)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session03/index.html">Session 3: LLMs for Data Annotation and Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture1.html">3.1 Zero-shot Learning with LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture2.html">3.2 Few-shot Learning and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture3.html">3.3 Comparing LLM Performance with Traditional Supervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session04/index.html">Session 4: Generative Explanations and Summaries in Social Science</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture1.html">4.1 Using LLMs for High-Quality Text Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture2.html">4.2 Social Bias Inference and Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture3.html">4.3 Figurative Language Explanation and Cultural Context</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session05/index.html">Session 5: Advanced Applications of LLMs in Social Science Research</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture1.html">5.1 Analyzing Large-Scale Textual Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture2.html">5.2 Misinformation and Fake News Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture3.html">5.3 Future Directions and Emerging Trends</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/edit/main/book/session01/lecture2.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/issues/new?title=Issue%20on%20page%20%2Fsession01/lecture2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/session01/lecture2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>1.2 Overview of Generative LLMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-generative-llms">1. Introduction to Generative LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-core-concept">Definition and core concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distinction-from-traditional-nlp-models">Distinction from traditional NLP models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-llms">2. Key Components of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-architecture">Transformer architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-attention mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-up-training-on-massive-datasets">Scaled-up training on massive datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notable-examples-of-llms">3. Notable Examples of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-generative-pre-trained-transformer-series">GPT (Generative Pre-trained Transformer) series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-and-its-variants">BERT and its variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-prominent-models">Other prominent models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#capabilities-of-llms-in-social-science-contexts">4. Capabilities of LLMs in Social Science Contexts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-and-completion">Text generation and completion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-answering-and-information-retrieval">Question answering and information retrieval</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summarization-and-paraphrasing">Summarization and paraphrasing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translation-and-cross-lingual-tasks">Translation and cross-lingual tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-and-emotion-detection">Sentiment analysis and emotion detection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process-of-llms">5. Training Process of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-on-large-corpora">Pre-training on large corpora</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-for-specific-tasks">Fine-tuning for specific tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-and-zero-shot-learning-capabilities">Few-shot and zero-shot learning capabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-llms-in-social-science-research">6. Advantages of LLMs in Social Science Research</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-complex-language-understanding-tasks">Handling complex language understanding tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ability-to-generate-human-like-text">Ability to generate human-like text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptability-to-various-domains-and-tasks">Adaptability to various domains and tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-challenges">7. Limitations and Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-biases-in-training-data">Potential biases in training data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lack-of-true-understanding-or-reasoning">Lack of true understanding or reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-resources-required">Computational resources required</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-considerations-in-deployment">Ethical considerations in deployment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recent-advancements">8. Recent Advancements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improvements-in-model-size-and-efficiency">Improvements in model size and efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-multi-modal-capabilities">Enhanced multi-modal capabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#progress-in-mitigating-biases-and-improving-factual-accuracy">Progress in mitigating biases and improving factual accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-social-science">9. Applications in Social Science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-annotation-and-classification">Data annotation and classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-explanations-and-summaries">Generating explanations and summaries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-large-scale-textual-data">Analyzing large-scale textual data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inferring-social-patterns-and-trends">Inferring social patterns and trends</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">10. Future Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-world-knowledge-and-physical-world-understanding">Integration with world knowledge and physical world understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-interpretability-and-transparency">Improved interpretability and transparency</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="overview-of-generative-llms">
<h1>1.2 Overview of Generative LLMs<a class="headerlink" href="#overview-of-generative-llms" title="Link to this heading">#</a></h1>
<section id="introduction-to-generative-llms">
<h2>1. Introduction to Generative LLMs<a class="headerlink" href="#introduction-to-generative-llms" title="Link to this heading">#</a></h2>
<p>Generative Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. These models represent a significant leap forward in natural language processing (NLP) technology.</p>
<section id="definition-and-core-concept">
<h3>Definition and core concept<a class="headerlink" href="#definition-and-core-concept" title="Link to this heading">#</a></h3>
<p>Generative LLMs are neural networks trained on vast amounts of text data to predict the next word in a sequence, allowing them to generate coherent and contextually appropriate text.</p>
</section>
<section id="distinction-from-traditional-nlp-models">
<h3>Distinction from traditional NLP models<a class="headerlink" href="#distinction-from-traditional-nlp-models" title="Link to this heading">#</a></h3>
<p>Unlike traditional NLP models that focus on specific tasks (e.g., sentiment analysis or named entity recognition), generative LLMs can perform a wide range of language tasks without task-specific training.</p>
<p>Example: While a traditional sentiment analysis model might only classify text as positive or negative, a generative LLM could analyze sentiment, explain the reasoning, and even rewrite the text to change its sentiment.</p>
</section>
</section>
<section id="key-components-of-llms">
<h2>2. Key Components of LLMs<a class="headerlink" href="#key-components-of-llms" title="Link to this heading">#</a></h2>
<section id="transformer-architecture">
<h3>Transformer architecture<a class="headerlink" href="#transformer-architecture" title="Link to this heading">#</a></h3>
<p>LLMs are built on the transformer architecture, which uses self-attention mechanisms to process input sequences in parallel, allowing for more efficient training on large datasets.</p>
</section>
<section id="self-attention-mechanism">
<h3>Self-attention mechanism<a class="headerlink" href="#self-attention-mechanism" title="Link to this heading">#</a></h3>
<p>Self-attention allows the model to weigh the importance of different words in a sentence when processing each word, capturing long-range dependencies in text.</p>
<p>Example: In the sentence “The animal didn’t cross the street because it was too wide,” self-attention helps the model understand that “it” refers to “the street” and not “the animal.”</p>
</section>
<section id="scaled-up-training-on-massive-datasets">
<h3>Scaled-up training on massive datasets<a class="headerlink" href="#scaled-up-training-on-massive-datasets" title="Link to this heading">#</a></h3>
<p>LLMs are trained on enormous text corpora, often containing hundreds of billions of words from diverse sources like websites, books, and articles.</p>
</section>
</section>
<section id="notable-examples-of-llms">
<h2>3. Notable Examples of LLMs<a class="headerlink" href="#notable-examples-of-llms" title="Link to this heading">#</a></h2>
<section id="gpt-generative-pre-trained-transformer-series">
<h3>GPT (Generative Pre-trained Transformer) series<a class="headerlink" href="#gpt-generative-pre-trained-transformer-series" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>GPT-3: 175 billion parameters, capable of generating human-like text across various domains</p></li>
<li><p>GPT-4: Latest iteration with multimodal capabilities (text and image input)</p></li>
</ul>
</section>
<section id="bert-and-its-variants">
<h3>BERT and its variants<a class="headerlink" href="#bert-and-its-variants" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>BERT (Bidirectional Encoder Representations from Transformers): Focuses on understanding context from both directions in a sentence</p></li>
<li><p>RoBERTa, ALBERT: Improved versions of BERT with different training strategies</p></li>
</ul>
</section>
<section id="other-prominent-models">
<h3>Other prominent models<a class="headerlink" href="#other-prominent-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>T5 (Text-to-Text Transfer Transformer): Frames all NLP tasks as text-to-text problems</p></li>
<li><p>DALL-E: Generates images from text descriptions</p></li>
</ul>
</section>
</section>
<section id="capabilities-of-llms-in-social-science-contexts">
<h2>4. Capabilities of LLMs in Social Science Contexts<a class="headerlink" href="#capabilities-of-llms-in-social-science-contexts" title="Link to this heading">#</a></h2>
<section id="text-generation-and-completion">
<h3>Text generation and completion<a class="headerlink" href="#text-generation-and-completion" title="Link to this heading">#</a></h3>
<p>LLMs can generate coherent paragraphs or complete partial text, useful for creating research hypotheses or expanding on ideas.</p>
<p>Example: Given the prompt “The impact of social media on political polarization,” an LLM could generate a detailed paragraph discussing various aspects of this topic.</p>
</section>
<section id="question-answering-and-information-retrieval">
<h3>Question answering and information retrieval<a class="headerlink" href="#question-answering-and-information-retrieval" title="Link to this heading">#</a></h3>
<p>LLMs can understand and answer complex questions, making them valuable for literature reviews or data exploration.</p>
<p>Example: Researchers could ask, “What are the main theories explaining the gender wage gap?” and receive a comprehensive answer with key points from various studies.</p>
</section>
<section id="summarization-and-paraphrasing">
<h3>Summarization and paraphrasing<a class="headerlink" href="#summarization-and-paraphrasing" title="Link to this heading">#</a></h3>
<p>LLMs can condense long texts or rephrase content, useful for processing large volumes of research papers or interview transcripts.</p>
</section>
<section id="translation-and-cross-lingual-tasks">
<h3>Translation and cross-lingual tasks<a class="headerlink" href="#translation-and-cross-lingual-tasks" title="Link to this heading">#</a></h3>
<p>These models can translate between languages and perform analysis across multiple languages, facilitating international research.</p>
</section>
<section id="sentiment-analysis-and-emotion-detection">
<h3>Sentiment analysis and emotion detection<a class="headerlink" href="#sentiment-analysis-and-emotion-detection" title="Link to this heading">#</a></h3>
<p>LLMs can identify and explain complex emotions and sentiments in text, going beyond simple positive/negative classifications.</p>
</section>
</section>
<section id="training-process-of-llms">
<h2>5. Training Process of LLMs<a class="headerlink" href="#training-process-of-llms" title="Link to this heading">#</a></h2>
<section id="pre-training-on-large-corpora">
<h3>Pre-training on large corpora<a class="headerlink" href="#pre-training-on-large-corpora" title="Link to this heading">#</a></h3>
<p>LLMs are initially trained on diverse text data to learn general language patterns and knowledge.</p>
</section>
<section id="fine-tuning-for-specific-tasks">
<h3>Fine-tuning for specific tasks<a class="headerlink" href="#fine-tuning-for-specific-tasks" title="Link to this heading">#</a></h3>
<p>Pre-trained models can be further trained on domain-specific data to adapt to particular research areas or tasks.</p>
</section>
<section id="few-shot-and-zero-shot-learning-capabilities">
<h3>Few-shot and zero-shot learning capabilities<a class="headerlink" href="#few-shot-and-zero-shot-learning-capabilities" title="Link to this heading">#</a></h3>
<p>LLMs can perform tasks with minimal (few-shot) or no (zero-shot) specific examples, adapting to new scenarios based on their general language understanding.</p>
<p>Example: A model could classify social media posts into themes it hasn’t been explicitly trained on, based on a few examples or just a description of the categories.</p>
</section>
</section>
<section id="advantages-of-llms-in-social-science-research">
<h2>6. Advantages of LLMs in Social Science Research<a class="headerlink" href="#advantages-of-llms-in-social-science-research" title="Link to this heading">#</a></h2>
<section id="handling-complex-language-understanding-tasks">
<h3>Handling complex language understanding tasks<a class="headerlink" href="#handling-complex-language-understanding-tasks" title="Link to this heading">#</a></h3>
<p>LLMs can grasp nuanced meanings, idiomatic expressions, and context-dependent interpretations.</p>
</section>
<section id="ability-to-generate-human-like-text">
<h3>Ability to generate human-like text<a class="headerlink" href="#ability-to-generate-human-like-text" title="Link to this heading">#</a></h3>
<p>This capability is useful for creating synthetic data, formulating research questions, or generating interview questions.</p>
</section>
<section id="adaptability-to-various-domains-and-tasks">
<h3>Adaptability to various domains and tasks<a class="headerlink" href="#adaptability-to-various-domains-and-tasks" title="Link to this heading">#</a></h3>
<p>A single LLM can be applied to multiple research areas, from analyzing historical texts to coding contemporary social media posts.</p>
</section>
</section>
<section id="limitations-and-challenges">
<h2>7. Limitations and Challenges<a class="headerlink" href="#limitations-and-challenges" title="Link to this heading">#</a></h2>
<section id="potential-biases-in-training-data">
<h3>Potential biases in training data<a class="headerlink" href="#potential-biases-in-training-data" title="Link to this heading">#</a></h3>
<p>LLMs may perpetuate or amplify biases present in their training data, requiring careful scrutiny in social science applications.</p>
</section>
<section id="lack-of-true-understanding-or-reasoning">
<h3>Lack of true understanding or reasoning<a class="headerlink" href="#lack-of-true-understanding-or-reasoning" title="Link to this heading">#</a></h3>
<p>Despite their sophisticated outputs, LLMs don’t truly “understand” text in a human sense and may produce plausible-sounding but incorrect information.</p>
</section>
<section id="computational-resources-required">
<h3>Computational resources required<a class="headerlink" href="#computational-resources-required" title="Link to this heading">#</a></h3>
<p>Training and running large LLMs require significant computational power, which may be a barrier for some researchers.</p>
</section>
<section id="ethical-considerations-in-deployment">
<h3>Ethical considerations in deployment<a class="headerlink" href="#ethical-considerations-in-deployment" title="Link to this heading">#</a></h3>
<p>Issues of privacy, consent, and the potential for misuse need to be carefully considered when applying LLMs to social science research.</p>
</section>
</section>
<section id="recent-advancements">
<h2>8. Recent Advancements<a class="headerlink" href="#recent-advancements" title="Link to this heading">#</a></h2>
<section id="improvements-in-model-size-and-efficiency">
<h3>Improvements in model size and efficiency<a class="headerlink" href="#improvements-in-model-size-and-efficiency" title="Link to this heading">#</a></h3>
<p>Newer models achieve better performance with fewer parameters, making them more accessible for research use.</p>
</section>
<section id="enhanced-multi-modal-capabilities">
<h3>Enhanced multi-modal capabilities<a class="headerlink" href="#enhanced-multi-modal-capabilities" title="Link to this heading">#</a></h3>
<p>Some LLMs can now process and generate both text and images, opening new possibilities for analyzing visual social media content or historical artifacts.</p>
</section>
<section id="progress-in-mitigating-biases-and-improving-factual-accuracy">
<h3>Progress in mitigating biases and improving factual accuracy<a class="headerlink" href="#progress-in-mitigating-biases-and-improving-factual-accuracy" title="Link to this heading">#</a></h3>
<p>Ongoing research aims to reduce biases and increase the reliability of LLM outputs, crucial for their use in academic research.</p>
</section>
</section>
<section id="applications-in-social-science">
<h2>9. Applications in Social Science<a class="headerlink" href="#applications-in-social-science" title="Link to this heading">#</a></h2>
<section id="data-annotation-and-classification">
<h3>Data annotation and classification<a class="headerlink" href="#data-annotation-and-classification" title="Link to this heading">#</a></h3>
<p>LLMs can assist in coding qualitative data, potentially increasing efficiency and consistency in analysis.</p>
<p>Example: Automatically categorizing open-ended survey responses into themes, with the ability to explain the reasoning behind each classification.</p>
</section>
<section id="generating-explanations-and-summaries">
<h3>Generating explanations and summaries<a class="headerlink" href="#generating-explanations-and-summaries" title="Link to this heading">#</a></h3>
<p>LLMs can provide detailed explanations of complex social phenomena or summarize key findings from multiple studies.</p>
</section>
<section id="analyzing-large-scale-textual-data">
<h3>Analyzing large-scale textual data<a class="headerlink" href="#analyzing-large-scale-textual-data" title="Link to this heading">#</a></h3>
<p>These models excel at processing and extracting insights from vast amounts of text, such as social media posts or historical documents.</p>
</section>
<section id="inferring-social-patterns-and-trends">
<h3>Inferring social patterns and trends<a class="headerlink" href="#inferring-social-patterns-and-trends" title="Link to this heading">#</a></h3>
<p>LLMs can identify emerging topics, shifts in public opinion, or cultural trends from large-scale text data.</p>
<p>Example: Analyzing years of social media data to track changes in public attitudes towards climate change.</p>
</section>
</section>
<section id="future-directions">
<h2>10. Future Directions<a class="headerlink" href="#future-directions" title="Link to this heading">#</a></h2>
<section id="integration-with-world-knowledge-and-physical-world-understanding">
<h3>Integration with world knowledge and physical world understanding<a class="headerlink" href="#integration-with-world-knowledge-and-physical-world-understanding" title="Link to this heading">#</a></h3>
<p>Future LLMs may better integrate factual knowledge and understanding of physical world constraints, improving their reliability for research applications.</p>
</section>
<section id="improved-interpretability-and-transparency">
<h3>Improved interpretability and transparency<a class="headerlink" href="#improved-interpretability-and-transparency" title="Link to this heading">#</a></h3>
<p>Ongoing research aims to make LLM decision-making processes more transparent, allowing researchers to better understand and validate their outputs.</p>
<p>As LLMs continue to evolve, they offer exciting possibilities for social science research, potentially transforming how we analyze text data, generate hypotheses, and explore complex social phenomena. However, their use also requires careful consideration of their limitations and ethical implications. Social scientists must approach these powerful tools with both enthusiasm and critical scrutiny, developing new methodologies to harness their capabilities while ensuring the validity and integrity of their research.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/nlp4ss",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./session01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1.1 Fundamentals of NLP and its Evolution</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">1.3 Ethical Considerations and Challenges in Using LLMs for Research</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-generative-llms">1. Introduction to Generative LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-core-concept">Definition and core concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distinction-from-traditional-nlp-models">Distinction from traditional NLP models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-llms">2. Key Components of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-architecture">Transformer architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-attention mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-up-training-on-massive-datasets">Scaled-up training on massive datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notable-examples-of-llms">3. Notable Examples of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-generative-pre-trained-transformer-series">GPT (Generative Pre-trained Transformer) series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-and-its-variants">BERT and its variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-prominent-models">Other prominent models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#capabilities-of-llms-in-social-science-contexts">4. Capabilities of LLMs in Social Science Contexts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-and-completion">Text generation and completion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-answering-and-information-retrieval">Question answering and information retrieval</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summarization-and-paraphrasing">Summarization and paraphrasing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translation-and-cross-lingual-tasks">Translation and cross-lingual tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-and-emotion-detection">Sentiment analysis and emotion detection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process-of-llms">5. Training Process of LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-training-on-large-corpora">Pre-training on large corpora</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-for-specific-tasks">Fine-tuning for specific tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-and-zero-shot-learning-capabilities">Few-shot and zero-shot learning capabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-llms-in-social-science-research">6. Advantages of LLMs in Social Science Research</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-complex-language-understanding-tasks">Handling complex language understanding tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ability-to-generate-human-like-text">Ability to generate human-like text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptability-to-various-domains-and-tasks">Adaptability to various domains and tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-challenges">7. Limitations and Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-biases-in-training-data">Potential biases in training data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lack-of-true-understanding-or-reasoning">Lack of true understanding or reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-resources-required">Computational resources required</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-considerations-in-deployment">Ethical considerations in deployment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recent-advancements">8. Recent Advancements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improvements-in-model-size-and-efficiency">Improvements in model size and efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-multi-modal-capabilities">Enhanced multi-modal capabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#progress-in-mitigating-biases-and-improving-factual-accuracy">Progress in mitigating biases and improving factual accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-social-science">9. Applications in Social Science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-annotation-and-classification">Data annotation and classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-explanations-and-summaries">Generating explanations and summaries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-large-scale-textual-data">Analyzing large-scale textual data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inferring-social-patterns-and-trends">Inferring social patterns and trends</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">10. Future Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-world-knowledge-and-physical-world-understanding">Integration with world knowledge and physical world understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-interpretability-and-transparency">Improved interpretability and transparency</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
