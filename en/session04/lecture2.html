
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.2 Social Bias Inference and Analysis &#8212; NLP for Social Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@10.2.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'session04/lecture2';</script>
    <script src="../_static/language_switcher.js?v=ff97be19"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.3 Figurative Language Explanation and Cultural Context" href="lecture3.html" />
    <link rel="prev" title="4.1 Using LLMs for High-Quality Text Generation" href="lecture1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">NLP for Social Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../session01/index.html">Session 1 - Introduction to NLP for Social Science</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture1.html">1.1 Fundamentals of NLP and its Evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture2.html">1.2 Overview of Generative LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture3.html">1.3 Ethical Considerations and Challenges in Using LLMs for Research</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session02/index.html">Session 2: Traditional NLP Techniques and Text Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture1.html">2.1 Text Cleaning, Normalization, and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture2.html">2.2 Basic NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session02/lecture3.html">2.3 Topic Modeling and Latent Dirichlet Allocation (LDA)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session03/index.html">Session 3: LLMs for Data Annotation and Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture1.html">3.1 Zero-shot Learning with LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture2.html">3.2 Few-shot Learning and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture3.html">3.3 Comparing LLM Performance with Traditional Supervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Session 4: Generative Explanations and Summaries in Social Science</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lecture1.html">4.1 Using LLMs for High-Quality Text Generation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">4.2 Social Bias Inference and Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture3.html">4.3 Figurative Language Explanation and Cultural Context</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session05/index.html">Session 5: Advanced Applications of LLMs in Social Science Research</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture1.html">5.1 Analyzing Large-Scale Textual Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture2.html">5.2 Misinformation and Fake News Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture3.html">5.3 Future Directions and Emerging Trends</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Course Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/edit/main/book/session04/lecture2.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/issues/new?title=Issue%20on%20page%20%2Fsession04/lecture2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/session04/lecture2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4.2 Social Bias Inference and Analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-social-bias-in-nlp">1. Introduction to Social Bias in NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sources-of-bias-in-llms">2. Sources of Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#techniques-for-detecting-bias-in-llms">3. Techniques for Detecting Bias in LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-association-test-weat">Word Embedding Association Test (WEAT)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-bias-in-llms">4. Quantifying Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#social-bias-inference-using-llms">5. Social Bias Inference Using LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-gender-bias">6. Analyzing Gender Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#racial-and-ethnic-bias-analysis">7. Racial and Ethnic Bias Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mitigating-bias-in-llms">8. Mitigating Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-considerations-and-challenges">9. Ethical Considerations and Challenges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="social-bias-inference-and-analysis">
<h1>4.2 Social Bias Inference and Analysis<a class="headerlink" href="#social-bias-inference-and-analysis" title="Link to this heading">#</a></h1>
<section id="introduction-to-social-bias-in-nlp">
<h2>1. Introduction to Social Bias in NLP<a class="headerlink" href="#introduction-to-social-bias-in-nlp" title="Link to this heading">#</a></h2>
<p>Social bias in Natural Language Processing (NLP) refers to the unfair or prejudiced treatment of individuals or groups based on attributes such as gender, race, age, or socioeconomic status. In the context of Large Language Models (LLMs), these biases can manifest in various ways, potentially impacting social science research outcomes.</p>
<div align="center" class="mermaid align-center">
            graph TD
    A[Social Bias in NLP] --&gt; B[Gender Bias]
    A --&gt; C[Racial Bias]
    A --&gt; D[Age Bias]
    A --&gt; E[Socioeconomic Bias]
    B --&gt; F[Occupational Stereotypes]
    B --&gt; G[Gendered Language]
    C --&gt; H[Ethnic Stereotypes]
    C --&gt; I[Cultural Bias]
    D --&gt; J[Ageism]
    D --&gt; K[Generational Stereotypes]
    E --&gt; L[Class-based Prejudice]
    E --&gt; M[Economic Discrimination]
        </div>
        <p>Understanding and addressing these biases is crucial for ensuring the validity and fairness of social science research that utilizes LLMs.</p>
</section>
<section id="sources-of-bias-in-llms">
<h2>2. Sources of Bias in LLMs<a class="headerlink" href="#sources-of-bias-in-llms" title="Link to this heading">#</a></h2>
<p>Biases in LLMs can originate from various sources:</p>
<ol class="arabic simple">
<li><p>Training data biases: Reflect societal biases present in the text used to train the models.</p></li>
<li><p>Algorithmic biases: Arise from the model architecture and training process.</p></li>
<li><p>Deployment and interpretation biases: Occur when models are applied in specific contexts or when their outputs are interpreted.</p></li>
</ol>
</section>
<section id="techniques-for-detecting-bias-in-llms">
<h2>3. Techniques for Detecting Bias in LLMs<a class="headerlink" href="#techniques-for-detecting-bias-in-llms" title="Link to this heading">#</a></h2>
<section id="word-embedding-association-test-weat">
<h3>Word Embedding Association Test (WEAT)<a class="headerlink" href="#word-embedding-association-test-weat" title="Link to this heading">#</a></h3>
<p>WEAT is a common method for detecting bias in word embeddings. Here’s an example implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">weat_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">target_set1</span><span class="p">,</span> <span class="n">target_set2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">):</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">target_set1</span> <span class="o">+</span> <span class="n">target_set2</span> <span class="o">+</span> <span class="n">attribute_set1</span> <span class="o">+</span> <span class="n">attribute_set2</span><span class="p">:</span>
        <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_embedding</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>

    <span class="n">association_1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
                        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">target_set1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">attribute_set1</span><span class="p">)</span>
    <span class="n">association_2</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
                        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">target_set2</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">attribute_set2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">association_1</span> <span class="o">-</span> <span class="n">association_2</span>

<span class="c1"># Example usage</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">target_set1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;boy&quot;</span><span class="p">,</span> <span class="s2">&quot;father&quot;</span><span class="p">,</span> <span class="s2">&quot;brother&quot;</span><span class="p">,</span> <span class="s2">&quot;son&quot;</span><span class="p">]</span>
<span class="n">target_set2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="s2">&quot;girl&quot;</span><span class="p">,</span> <span class="s2">&quot;mother&quot;</span><span class="p">,</span> <span class="s2">&quot;sister&quot;</span><span class="p">,</span> <span class="s2">&quot;daughter&quot;</span><span class="p">]</span>
<span class="n">attribute_set1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;computer&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;scientist&quot;</span><span class="p">]</span>
<span class="n">attribute_set2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nurse&quot;</span><span class="p">,</span> <span class="s2">&quot;teacher&quot;</span><span class="p">,</span> <span class="s2">&quot;artist&quot;</span><span class="p">]</span>

<span class="n">weat_score</span> <span class="o">=</span> <span class="n">weat_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">target_set1</span><span class="p">,</span> <span class="n">target_set2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WEAT score: </span><span class="si">{</span><span class="n">weat_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This example calculates the WEAT score for gender bias in occupations. A positive score indicates bias towards associating male terms with technical professions and female terms with caregiving professions.</p>
</section>
</section>
<section id="quantifying-bias-in-llms">
<h2>4. Quantifying Bias in LLMs<a class="headerlink" href="#quantifying-bias-in-llms" title="Link to this heading">#</a></h2>
<p>To quantify bias, we can use metrics like the WEAT score shown above. Additionally, we can analyze the statistical significance of these scores:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>

<span class="k">def</span> <span class="nf">weat_significance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">target_set1</span><span class="p">,</span> <span class="n">target_set2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">,</span> <span class="n">num_permutations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">observed_score</span> <span class="o">=</span> <span class="n">weat_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">target_set1</span><span class="p">,</span> <span class="n">target_set2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">)</span>

    <span class="n">all_targets</span> <span class="o">=</span> <span class="n">target_set1</span> <span class="o">+</span> <span class="n">target_set2</span>
    <span class="n">permutation_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_permutations</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">all_targets</span><span class="p">)</span>
        <span class="n">perm_target1</span> <span class="o">=</span> <span class="n">all_targets</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">target_set1</span><span class="p">)]</span>
        <span class="n">perm_target2</span> <span class="o">=</span> <span class="n">all_targets</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">target_set1</span><span class="p">):]</span>
        <span class="n">perm_score</span> <span class="o">=</span> <span class="n">weat_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">perm_target1</span><span class="p">,</span> <span class="n">perm_target2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">)</span>
        <span class="n">permutation_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perm_score</span><span class="p">)</span>

    <span class="n">p_value</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">score</span> <span class="o">&gt;=</span> <span class="n">observed_score</span> <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">permutation_scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_permutations</span>
    <span class="k">return</span> <span class="n">observed_score</span><span class="p">,</span> <span class="n">p_value</span>

<span class="c1"># Example usage</span>
<span class="n">observed_score</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">weat_significance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">target_set1</span><span class="p">,</span> <span class="n">target_set2</span><span class="p">,</span> <span class="n">attribute_set1</span><span class="p">,</span> <span class="n">attribute_set2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Observed WEAT score: </span><span class="si">{</span><span class="n">observed_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This code calculates the statistical significance of the WEAT score using permutation testing.</p>
</section>
<section id="social-bias-inference-using-llms">
<h2>5. Social Bias Inference Using LLMs<a class="headerlink" href="#social-bias-inference-using-llms" title="Link to this heading">#</a></h2>
<p>We can use LLMs themselves to infer social biases through carefully crafted prompts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>

<span class="k">def</span> <span class="nf">generate_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">analyze_occupation_bias</span><span class="p">(</span><span class="n">occupation</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">occupation</span><span class="si">}</span><span class="s2"> walked into the room. He&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">occupation</span><span class="si">}</span><span class="s2"> walked into the room. She&quot;</span>
    <span class="p">]</span>

    <span class="n">completions</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">completions</span>

<span class="c1"># Example usage</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">occupation</span> <span class="o">=</span> <span class="s2">&quot;engineer&quot;</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">analyze_occupation_bias</span><span class="p">(</span><span class="n">occupation</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Completions for &#39;</span><span class="si">{</span><span class="n">occupation</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Male prompt: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Female prompt: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This example generates completions for prompts involving different genders in a specific occupation, allowing us to analyze potential gender biases in the model’s output.</p>
</section>
<section id="analyzing-gender-bias">
<h2>6. Analyzing Gender Bias<a class="headerlink" href="#analyzing-gender-bias" title="Link to this heading">#</a></h2>
<p>To analyze gender bias more systematically, we can create a function to compare word associations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gender_association_test</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">male_term</span><span class="p">,</span> <span class="n">female_term</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">male_associations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">female_associations</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">male_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> is a </span><span class="si">{</span><span class="n">male_term</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">female_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> is a </span><span class="si">{</span><span class="n">female_term</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">male_prob</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">male_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">male_term</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">female_prob</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">female_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">female_term</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">male_associations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">male_prob</span><span class="p">)</span>
        <span class="n">female_associations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">female_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">male_associations</span><span class="p">,</span> <span class="n">female_associations</span>

<span class="c1"># Example usage</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;doctor&quot;</span><span class="p">,</span> <span class="s2">&quot;nurse&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;teacher&quot;</span><span class="p">,</span> <span class="s2">&quot;scientist&quot;</span><span class="p">,</span> <span class="s2">&quot;artist&quot;</span><span class="p">]</span>
<span class="n">male_term</span> <span class="o">=</span> <span class="s2">&quot;man&quot;</span>
<span class="n">female_term</span> <span class="o">=</span> <span class="s2">&quot;woman&quot;</span>

<span class="n">male_assoc</span><span class="p">,</span> <span class="n">female_assoc</span> <span class="o">=</span> <span class="n">gender_association_test</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">male_term</span><span class="p">,</span> <span class="n">female_term</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">male_prob</span><span class="p">,</span> <span class="n">female_prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">male_assoc</span><span class="p">,</span> <span class="n">female_assoc</span><span class="p">):</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">male_prob</span> <span class="o">-</span> <span class="n">female_prob</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: Male association: </span><span class="si">{</span><span class="n">male_prob</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Female association: </span><span class="si">{</span><span class="n">female_prob</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Bias: </span><span class="si">{</span><span class="n">bias</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This function calculates the association probabilities between occupations and gender terms, helping to quantify gender bias in occupational stereotypes.</p>
</section>
<section id="racial-and-ethnic-bias-analysis">
<h2>7. Racial and Ethnic Bias Analysis<a class="headerlink" href="#racial-and-ethnic-bias-analysis" title="Link to this heading">#</a></h2>
<p>To analyze racial and ethnic biases, we can use a similar approach with culturally associated names:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">name_sentiment_analysis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">sentiments</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is a&quot;</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">generate_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="n">sentiments</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">completion</span>
    <span class="k">return</span> <span class="n">sentiments</span>

<span class="c1"># Example usage</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Emily&quot;</span><span class="p">,</span> <span class="s2">&quot;Lakisha&quot;</span><span class="p">,</span> <span class="s2">&quot;Brad&quot;</span><span class="p">,</span> <span class="s2">&quot;Jamal&quot;</span><span class="p">,</span> <span class="s2">&quot;Zhang Wei&quot;</span><span class="p">,</span> <span class="s2">&quot;Sven&quot;</span><span class="p">,</span> <span class="s2">&quot;Mohammed&quot;</span><span class="p">]</span>
<span class="n">name_sentiments</span> <span class="o">=</span> <span class="n">name_sentiment_analysis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="n">name_sentiments</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This function generates completions for prompts starting with different names, allowing us to analyze potential racial or ethnic biases in the model’s associations.</p>
</section>
<section id="mitigating-bias-in-llms">
<h2>8. Mitigating Bias in LLMs<a class="headerlink" href="#mitigating-bias-in-llms" title="Link to this heading">#</a></h2>
<p>While completely eliminating bias is challenging, there are techniques to mitigate it. One approach is to use debiasing prompts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">debiased_generation</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">debiasing_prefixes</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">debiased_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="n">debiasing_prefixes</span><span class="p">]</span>
    <span class="n">completions</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_completion</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">debiased_prompts</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">completions</span>

<span class="c1"># Example usage</span>
<span class="n">debiasing_prefixes</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Considering all genders equally,&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Without any racial stereotypes,&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Regardless of socioeconomic background,&quot;</span>
<span class="p">]</span>

<span class="n">biased_prompt</span> <span class="o">=</span> <span class="s2">&quot;The CEO of the company is&quot;</span>
<span class="n">debiased_results</span> <span class="o">=</span> <span class="n">debiased_generation</span><span class="p">(</span><span class="n">biased_prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">debiasing_prefixes</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Debiased completions:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">completion</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">debiasing_prefixes</span><span class="p">,</span> <span class="n">debiased_results</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">completion</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This approach uses explicit debiasing prefixes to guide the model towards more neutral completions.</p>
</section>
<section id="ethical-considerations-and-challenges">
<h2>9. Ethical Considerations and Challenges<a class="headerlink" href="#ethical-considerations-and-challenges" title="Link to this heading">#</a></h2>
<p>When conducting social bias analysis, researchers must consider several ethical issues:</p>
<ol class="arabic simple">
<li><p>Avoid reinforcing stereotypes through analysis</p></li>
<li><p>Ensure privacy and consent when using real-world data</p></li>
<li><p>Acknowledge the cultural relativity of biases</p></li>
<li><p>Recognize the evolving nature of social biases</p></li>
</ol>
<div align="center" class="mermaid align-center">
            graph TD
    A[Ethical Considerations] --&gt; B[Avoid Reinforcing Stereotypes]
    A --&gt; C[Ensure Privacy and Consent]
    A --&gt; D[Acknowledge Cultural Relativity]
    A --&gt; E[Recognize Evolving Nature of Biases]
    B --&gt; F[Careful Reporting of Results]
    C --&gt; G[Anonymization Techniques]
    D --&gt; H[Cross-cultural Validation]
    E --&gt; I[Longitudinal Bias Studies]
        </div>
        </section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Social bias inference and analysis in LLMs is a critical area for social science researchers using these models. By employing techniques such as WEAT, prompt-based analysis, and careful examination of model outputs, researchers can detect and quantify various types of social biases. However, it’s important to approach this analysis with caution, considering ethical implications and the complex nature of social biases.</p>
<p>As the field progresses, we can expect more sophisticated techniques for bias detection and mitigation. Researchers should stay informed about the latest developments and best practices in this rapidly evolving area. By doing so, they can ensure more fair and accurate use of LLMs in social science research, ultimately contributing to a more equitable understanding of social phenomena.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/nlp4ss",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./session04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">4.1 Using LLMs for High-Quality Text Generation</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">4.3 Figurative Language Explanation and Cultural Context</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-social-bias-in-nlp">1. Introduction to Social Bias in NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sources-of-bias-in-llms">2. Sources of Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#techniques-for-detecting-bias-in-llms">3. Techniques for Detecting Bias in LLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-association-test-weat">Word Embedding Association Test (WEAT)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-bias-in-llms">4. Quantifying Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#social-bias-inference-using-llms">5. Social Bias Inference Using LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-gender-bias">6. Analyzing Gender Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#racial-and-ethnic-bias-analysis">7. Racial and Ethnic Bias Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mitigating-bias-in-llms">8. Mitigating Bias in LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-considerations-and-challenges">9. Ethical Considerations and Challenges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
