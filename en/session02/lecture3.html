
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2.3 Topic Modeling and Latent Dirichlet Allocation (LDA) &#8212; NLP for Social Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@10.2.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'session02/lecture3';</script>
    <script src="../_static/language_switcher.js?v=ff97be19"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Session 3: LLMs for Data Annotation and Classification" href="../session03/index.html" />
    <link rel="prev" title="2.2 Basic NLP Tasks" href="lecture2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">NLP for Social Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../session01/index.html">Session 1 - Introduction to NLP for Social Science</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture1.html">1.1 Fundamentals of NLP and its Evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture2.html">1.2 Overview of Generative LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session01/lecture3.html">1.3 Ethical Considerations and Challenges in Using LLMs for Research</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Session 2: Traditional NLP Techniques and Text Preprocessing</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lecture1.html">2.1 Text Cleaning, Normalization, and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture2.html">2.2 Basic NLP Tasks</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2.3 Topic Modeling and Latent Dirichlet Allocation (LDA)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session03/index.html">Session 3: LLMs for Data Annotation and Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture1.html">3.1 Zero-shot Learning with LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture2.html">3.2 Few-shot Learning and Prompt Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session03/lecture3.html">3.3 Comparing LLM Performance with Traditional Supervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session04/index.html">Session 4: Generative Explanations and Summaries in Social Science</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture1.html">4.1 Using LLMs for High-Quality Text Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture2.html">4.2 Social Bias Inference and Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session04/lecture3.html">4.3 Figurative Language Explanation and Cultural Context</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../session05/index.html">Session 5: Advanced Applications of LLMs in Social Science Research</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture1.html">5.1 Analyzing Large-Scale Textual Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture2.html">5.2 Misinformation and Fake News Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session05/lecture3.html">5.3 Future Directions and Emerging Trends</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Course Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/edit/main/book/session02/lecture3.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/nlp4ss/issues/new?title=Issue%20on%20page%20%2Fsession02/lecture3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/session02/lecture3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>2.3 Topic Modeling and Latent Dirichlet Allocation (LDA)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-topic-modeling">1. Introduction to Topic Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-latent-dirichlet-allocation-lda">2. Fundamentals of Latent Dirichlet Allocation (LDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation-of-lda">3. Mathematical Foundation of LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-algorithm">4. LDA Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-lda">5. Preparing Data for LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-lda">6. Implementing LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-lda-results">7. Interpreting LDA Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-topic-models">8. Evaluating Topic Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topic-modeling-techniques">9. Advanced Topic Modeling Techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-social-science-research">10. Applications in Social Science Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations-of-lda">11. Challenges and Limitations of LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-topic-modeling-with-other-nlp-techniques">12. Combining Topic Modeling with Other NLP Techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions-in-topic-modeling">13. Future Directions in Topic Modeling</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="topic-modeling-and-latent-dirichlet-allocation-lda">
<h1>2.3 Topic Modeling and Latent Dirichlet Allocation (LDA)<a class="headerlink" href="#topic-modeling-and-latent-dirichlet-allocation-lda" title="Link to this heading">#</a></h1>
<section id="introduction-to-topic-modeling">
<h2>1. Introduction to Topic Modeling<a class="headerlink" href="#introduction-to-topic-modeling" title="Link to this heading">#</a></h2>
<p>Topic modeling is a statistical method for discovering abstract topics that occur in a collection of documents. It’s an unsupervised machine learning technique that can automatically identify themes or topics within a large corpus of text.</p>
<p>Applications in social science research include:</p>
<ul class="simple">
<li><p>Analyzing trends in social media discussions</p></li>
<li><p>Identifying themes in survey responses</p></li>
<li><p>Exploring patterns in historical documents</p></li>
<li><p>Understanding policy documents and political discourse</p></li>
</ul>
<p>The most popular topic modeling approach is Latent Dirichlet Allocation (LDA), which we’ll focus on in this section.</p>
</section>
<section id="fundamentals-of-latent-dirichlet-allocation-lda">
<h2>2. Fundamentals of Latent Dirichlet Allocation (LDA)<a class="headerlink" href="#fundamentals-of-latent-dirichlet-allocation-lda" title="Link to this heading">#</a></h2>
<p>LDA is a generative probabilistic model that assumes each document is a mixture of a small number of topics, and each word’s presence is attributable to one of the document’s topics.</p>
<p>Key assumptions of LDA:</p>
<ol class="arabic simple">
<li><p>Documents are represented as random mixtures over latent topics</p></li>
<li><p>Each topic is characterized by a distribution over words</p></li>
</ol>
<p>The intuition behind LDA is that documents exhibit multiple topics in different proportions. For example, a news article about a political debate on climate change might be 60% about politics, 30% about climate science, and 10% about economics.</p>
</section>
<section id="mathematical-foundation-of-lda">
<h2>3. Mathematical Foundation of LDA<a class="headerlink" href="#mathematical-foundation-of-lda" title="Link to this heading">#</a></h2>
<p>LDA uses two main probability distributions:</p>
<ol class="arabic simple">
<li><p>Dirichlet distribution: Used to generate topic distributions for documents and word distributions for topics</p></li>
<li><p>Multinomial distribution: Used to generate words in documents based on the topic distributions</p></li>
</ol>
<p>The plate notation for LDA visualizes these relationships:</p>
<div align="center" class="mermaid align-center">
            graph LR
    A[α] --&gt; B[θ]
    B --&gt; C[z]
    C --&gt; D[w]
    E[φ] --&gt; D
    F[β] --&gt; E
    B -.-&gt; G[M]
    C -.-&gt; H[N]
    style G fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
        </div>
        <p>This diagram represents the plate notation for LDA, where:</p>
<ul class="simple">
<li><p>α is the parameter of the Dirichlet prior on the per-document topic distributions</p></li>
<li><p>β is the parameter of the Dirichlet prior on the per-topic word distribution</p></li>
<li><p>θ is the topic distribution for document d</p></li>
<li><p>φ is the word distribution for topic k</p></li>
<li><p>z is the topic for the n-th word in document d</p></li>
<li><p>w is the specific word</p></li>
<li><p>M represents the plate for documents</p></li>
<li><p>N represents the plate for words within each document</p></li>
</ul>
<p>The dotted lines to M and N indicate that these are plates (repeated elements) in the model.</p>
<p>Where:</p>
<ul class="simple">
<li><p>α is the parameter of the Dirichlet prior on the per-document topic distributions</p></li>
<li><p>β is the parameter of the Dirichlet prior on the per-topic word distribution</p></li>
<li><p>θ is the topic distribution for document d</p></li>
<li><p>φ is the word distribution for topic k</p></li>
<li><p>z is the topic for the n-th word in document d</p></li>
<li><p>w is the specific word</p></li>
</ul>
</section>
<section id="lda-algorithm">
<h2>4. LDA Algorithm<a class="headerlink" href="#lda-algorithm" title="Link to this heading">#</a></h2>
<p>The generative process for LDA:</p>
<ol class="arabic simple">
<li><p>For each topic k:</p>
<ul class="simple">
<li><p>Draw a word distribution φk ~ Dirichlet(β)</p></li>
</ul>
</li>
<li><p>For each document d:</p>
<ul class="simple">
<li><p>Draw a topic distribution θd ~ Dirichlet(α)</p></li>
<li><p>For each word position i in document d:</p>
<ul>
<li><p>Draw a topic zd,i ~ Multinomial(θd)</p></li>
<li><p>Draw a word wd,i ~ Multinomial(φzd,i)</p></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The inference problem in LDA is to reverse this process: given the observed words in documents, we want to infer the hidden topic structure.</p>
<p>Common inference techniques include:</p>
<ul class="simple">
<li><p>Variational inference</p></li>
<li><p>Gibbs sampling</p></li>
</ul>
</section>
<section id="preparing-data-for-lda">
<h2>5. Preparing Data for LDA<a class="headerlink" href="#preparing-data-for-lda" title="Link to this heading">#</a></h2>
<p>Before applying LDA, we need to preprocess our text data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Convert to lowercase and remove special characters</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-zA-Z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

    <span class="c1"># Tokenize</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># Remove stopwords and short words</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">]</span>

    <span class="c1"># Lemmatize</span>
    <span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">tokens</span>

<span class="c1"># Example usage</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The cat sat on the mat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The dog chased the cat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The mat was new and clean&quot;</span>
<span class="p">]</span>

<span class="n">processed_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess_text</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">processed_docs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="implementing-lda">
<h2>6. Implementing LDA<a class="headerlink" href="#implementing-lda" title="Link to this heading">#</a></h2>
<p>We’ll use the gensim library to implement LDA:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span>
<span class="kn">from</span> <span class="nn">gensim.models.ldamodel</span> <span class="kn">import</span> <span class="n">LdaModel</span>

<span class="c1"># Create dictionary</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">processed_docs</span><span class="p">)</span>

<span class="c1"># Create corpus</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">processed_docs</span><span class="p">]</span>

<span class="c1"># Train LDA model</span>
<span class="n">lda_model</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Print topics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Topics:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Get topic distribution for a document</span>
<span class="n">doc_lda</span> <span class="o">=</span> <span class="n">lda_model</span><span class="p">[</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Topic distribution for first document:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">topic</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">doc_lda</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="interpreting-lda-results">
<h2>7. Interpreting LDA Results<a class="headerlink" href="#interpreting-lda-results" title="Link to this heading">#</a></h2>
<p>To visualize LDA results, we can use pyLDAvis:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyLDAvis.gensim</span>

<span class="c1"># Prepare visualization</span>
<span class="n">vis_data</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">gensim</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">lda_model</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">)</span>

<span class="c1"># Save visualization to HTML file</span>
<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">vis_data</span><span class="p">,</span> <span class="s1">&#39;lda_visualization.html&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Visualization saved to &#39;lda_visualization.html&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This visualization helps in understanding:</p>
<ul class="simple">
<li><p>The prevalence of each topic</p></li>
<li><p>The most relevant terms for each topic</p></li>
<li><p>The relationships between topics</p></li>
</ul>
</section>
<section id="evaluating-topic-models">
<h2>8. Evaluating Topic Models<a class="headerlink" href="#evaluating-topic-models" title="Link to this heading">#</a></h2>
<p>We can evaluate topic models using coherence scores:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models.coherencemodel</span> <span class="kn">import</span> <span class="n">CoherenceModel</span>

<span class="c1"># Calculate coherence score</span>
<span class="n">coherence_model</span> <span class="o">=</span> <span class="n">CoherenceModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">lda_model</span><span class="p">,</span> <span class="n">texts</span><span class="o">=</span><span class="n">processed_docs</span><span class="p">,</span> <span class="n">dictionary</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
<span class="n">coherence_score</span> <span class="o">=</span> <span class="n">coherence_model</span><span class="o">.</span><span class="n">get_coherence</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coherence Score: </span><span class="si">{</span><span class="n">coherence_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>A higher coherence score generally indicates better topic quality.</p>
</section>
<section id="advanced-topic-modeling-techniques">
<h2>9. Advanced Topic Modeling Techniques<a class="headerlink" href="#advanced-topic-modeling-techniques" title="Link to this heading">#</a></h2>
<p>While LDA is the most common topic modeling technique, there are more advanced methods:</p>
<ol class="arabic simple">
<li><p>Dynamic Topic Models: For analyzing how topics evolve over time</p></li>
<li><p>Hierarchical LDA: Organizes topics into a hierarchy</p></li>
<li><p>Correlated Topic Models: Allows for correlation between topics</p></li>
</ol>
</section>
<section id="applications-in-social-science-research">
<h2>10. Applications in Social Science Research<a class="headerlink" href="#applications-in-social-science-research" title="Link to this heading">#</a></h2>
<p>Example: Analyzing trends in social media data</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">gensim.models.ldamodel</span> <span class="kn">import</span> <span class="n">LdaModel</span>

<span class="c1"># Assume we have a DataFrame &#39;df&#39; with columns &#39;date&#39; and &#39;text&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;social_media_data.csv&#39;</span><span class="p">)</span>

<span class="c1"># Preprocess texts</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;processed_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">preprocess_text</span><span class="p">)</span>

<span class="c1"># Create dictionary and corpus</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;processed_text&#39;</span><span class="p">])</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;processed_text&#39;</span><span class="p">]]</span>

<span class="c1"># Train LDA model</span>
<span class="n">lda_model</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Function to get dominant topic</span>
<span class="k">def</span> <span class="nf">get_dominant_topic</span><span class="p">(</span><span class="n">lda_result</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">lda_result</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Add dominant topic to DataFrame</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;dominant_topic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_dominant_topic</span><span class="p">(</span><span class="n">lda_model</span><span class="p">[</span><span class="n">doc</span><span class="p">])</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

<span class="c1"># Analyze topic trends over time</span>
<span class="n">topic_trends</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;dominant_topic&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>
<span class="n">topic_trends</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;line&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This example demonstrates how to use LDA to analyze trends in social media data over time.</p>
</section>
<section id="challenges-and-limitations-of-lda">
<h2>11. Challenges and Limitations of LDA<a class="headerlink" href="#challenges-and-limitations-of-lda" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Determining the optimal number of topics: This often requires experimentation and domain expertise.</p></li>
<li><p>Handling short texts: LDA typically performs poorly on very short documents (e.g., tweets).</p></li>
<li><p>Interpretability: Topics may not always be easily interpretable or meaningful to humans.</p></li>
</ol>
</section>
<section id="combining-topic-modeling-with-other-nlp-techniques">
<h2>12. Combining Topic Modeling with Other NLP Techniques<a class="headerlink" href="#combining-topic-modeling-with-other-nlp-techniques" title="Link to this heading">#</a></h2>
<p>Example: Combining topic modeling with sentiment analysis</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">textblob</span> <span class="kn">import</span> <span class="n">TextBlob</span>

<span class="c1"># Function to get sentiment</span>
<span class="k">def</span> <span class="nf">get_sentiment</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">sentiment</span><span class="o">.</span><span class="n">polarity</span>

<span class="c1"># Add sentiment to DataFrame</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">get_sentiment</span><span class="p">)</span>

<span class="c1"># Analyze sentiment by topic</span>
<span class="n">sentiment_by_topic</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;dominant_topic&#39;</span><span class="p">)[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average sentiment by topic:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment_by_topic</span><span class="p">)</span>
</pre></div>
</div>
<p>This example shows how to combine topic modeling with sentiment analysis to understand the emotional tone associated with different topics.</p>
</section>
<section id="future-directions-in-topic-modeling">
<h2>13. Future Directions in Topic Modeling<a class="headerlink" href="#future-directions-in-topic-modeling" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Neural topic models: Incorporating deep learning techniques for improved performance</p></li>
<li><p>Incorporating word embeddings: Using pre-trained word vectors to enhance topic coherence</p></li>
<li><p>Cross-lingual topic modeling: Developing models that can work across multiple languages</p></li>
</ol>
<p>In conclusion, topic modeling, particularly LDA, is a powerful tool for uncovering latent themes in large text corpora. It has wide-ranging applications in social science research, from analyzing social media trends to exploring historical documents. While LDA has some limitations, ongoing research is addressing these challenges and developing more sophisticated topic modeling techniques.</p>
<p>When using topic modeling in your research, remember to:</p>
<ul class="simple">
<li><p>Carefully preprocess your data</p></li>
<li><p>Experiment with different numbers of topics</p></li>
<li><p>Use both quantitative metrics (like coherence scores) and qualitative assessment to evaluate your models</p></li>
<li><p>Consider the specific needs of your research question when interpreting and applying the results</p></li>
</ul>
<p>By thoughtfully applying topic modeling techniques, social scientists can gain valuable insights from large-scale text data, uncovering patterns and themes that might be difficult to detect through manual analysis alone.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/nlp4ss",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./session02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">2.2 Basic NLP Tasks</p>
      </div>
    </a>
    <a class="right-next"
       href="../session03/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Session 3: LLMs for Data Annotation and Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-topic-modeling">1. Introduction to Topic Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-latent-dirichlet-allocation-lda">2. Fundamentals of Latent Dirichlet Allocation (LDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation-of-lda">3. Mathematical Foundation of LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-algorithm">4. LDA Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-lda">5. Preparing Data for LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-lda">6. Implementing LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-lda-results">7. Interpreting LDA Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-topic-models">8. Evaluating Topic Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topic-modeling-techniques">9. Advanced Topic Modeling Techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-social-science-research">10. Applications in Social Science Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations-of-lda">11. Challenges and Limitations of LDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-topic-modeling-with-other-nlp-techniques">12. Combining Topic Modeling with Other NLP Techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions-in-topic-modeling">13. Future Directions in Topic Modeling</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
